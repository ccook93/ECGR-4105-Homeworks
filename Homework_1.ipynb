{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39da065e-8863-493c-9a78-0b4e97b4bab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student: Casey Cook, ID#: 801085944, Homework 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9dd8fc-05dc-4b63-b74c-0851001b57fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### PROBLEM 1 ##############################################################################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d78da9-7466-4e79-9baf-120036a1e8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(x, y, theta): # Compute cost for linear regression.\n",
    "\n",
    "    predictions = x.dot(theta)\n",
    "    errors = np.subtract(predictions, y)\n",
    "    sqrErrors = np.square(errors)\n",
    "    J = 1 / (2 * m) * np.sum(sqrErrors) # cost function, sum of squared errors\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d1262b-8537-48d9-b1f6-abb73fc21348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, theta, alpha, iterations): # Compute cost for linear regression.\n",
    "\n",
    "    cost_history = np.zeros(iterations)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        predictions = x.dot(theta)\n",
    "        errors = np.subtract(predictions, y)\n",
    "        sum_delta = (alpha / m) * x.transpose().dot(errors);\n",
    "        theta = theta - sum_delta;\n",
    "        cost_history[i] = compute_cost(x, y, theta)\n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bbcb12-b782-4007-898b-d369d864e185",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = pd.read_csv('Housing.csv')\n",
    "#housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda71ce0-00a7-4d2f-899a-b59bd0ea9ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(housing)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ce6165-90b7-473f-af24-4f1b282126e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069df125-7d9e-44da-82f1-202aa9e32e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab10655-d429-45c5-8293-ee773b1031e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c435599f-3ef2-4665-96a5-36fe378fb089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can see that your dataset has many columns with values as 'Yes' or 'No'.\n",
    "# But in order to fit a regression line, we would need numerical values and not string.\n",
    "\n",
    "# List of variables to map\n",
    "\n",
    "# Initial code given wiht the assignment, will not work with this dataset.\n",
    "varlist = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning'] \n",
    "\n",
    "# a) Develop gradient descent training and evaluation code that predicts housing prices based on these input variables\n",
    "varlist1 = ['area','bedrooms','bathrooms','stories','parking']\n",
    "\n",
    "# b) Develop gradient descent the predits housing price based on the following variables.\n",
    "varlist2 = ['area', 'bedrooms', 'bathrooms', 'stories', 'mainroad', 'guestroom', 'basement','hotwaterheating', 'airconditioning', 'parking', 'prefarea']\n",
    "\n",
    "\n",
    "# Defining the map function\n",
    "def binary_map(x):\n",
    "    return x.map({'yes': 1, \"no\": 0})\n",
    "           \n",
    "    # Applying the function to the housing list\n",
    "    housing[varlist1] = housing[varlist].apply(binary_map)\n",
    "           \n",
    "# Check the housing dataframe now\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41afc39-7555-4233-8fce-a7bcd30c3cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the Data into Training and Testing Sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# We specify this so that the train and test data set always have the same rows, respectively. (columns?)\n",
    "np.random.seed(0)\n",
    "df_train, df_test = train_test_split(housing, test_size = 0.3, train_size = 0.7, random_state = 1 )\n",
    "\n",
    "mTrain = len(df_train)\n",
    "print('Length of training set:', mTrain)\n",
    "print('Dimensions of training set:',df_train.shape)\n",
    "# print(df_train[: 1]) # prints first 2 rows of training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6679c0c0-3c9f-4808-80c6-b8e98638e211",
   "metadata": {},
   "outputs": [],
   "source": [
    "mTest = len(df_test)\n",
    "print('Length of test set:',mTest)\n",
    "print('Dimensions of test set:',df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baf7f82-c278-413b-b782-9ef0bae3294b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a data set Newtrain and Newtest based on the following variables\n",
    "# This is were the training columns come into play, This can be used to define multiple data sets.\n",
    "\n",
    "num_vars = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking','price']\n",
    "\n",
    "varlist1 = ['area','bedrooms','bathrooms','stories','parking','price']\n",
    "varlistTest = ['area','bedrooms','bathrooms','stories','parking','price']\n",
    "\n",
    "varlist2 = ['area', 'bedrooms', 'bathrooms', 'stories', 'mainroad', 'guestroom', 'basement','hotwaterheating', 'airconditioning', 'parking', 'prefarea']\n",
    "\n",
    "df_Newtrain = df_train[varlist1]\n",
    "# df_Newtrain = df_train[num_vars] #this is the default list that is the baseline of this code\n",
    "\n",
    "df_Newtest = df_test[varlistTest]\n",
    "\n",
    "df_Newtrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f56532-fa7d-4186-930a-0b91cf57efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Newtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1b3ee4-3e26-4980-829b-1828a6fa58ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Newtest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b091345d-05e3-4a5e-996c-506067cb4513",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Newtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c08618b-3f4f-4c38-a8e2-20c99d478234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section will be used to create the regression gradient descent algorithm w/o normalization or standardization\n",
    "# df_Newtrain and df_Newtest will be used, run linear regression and grasdient descent on both\n",
    "# Should this be a multivariable gradient descent algorithm? GET OTHER VALUES FROM df_Newtrain!!!!!!!\n",
    "\n",
    "# Training set evaluation (df_Newtrain)\n",
    "x1 = df_Newtrain.values[:, 0] # get input values from first column\n",
    "x2 = df_Newtrain.values[:, 1] # get output values from second column\n",
    "x3 = df_Newtrain.values[:, 2]\n",
    "x4 = df_Newtrain.values[:, 3]\n",
    "x5 = df_Newtrain.values[:, 4]\n",
    "Y = df_Newtrain.values[:, 5]\n",
    "\n",
    "# These values were very large, so I scaled them so that all of the data points work with each other better\n",
    "Ynorm = np.multiply(Y,0.00001) # scales the price my Price*(10^-5)\n",
    "x1Norm = np.multiply(x1,0.001) # scales the area by Area*(10^-3)\n",
    "\n",
    "m = len(x1) # Number of training examples\n",
    "\n",
    "#print('x1 = ', x1[: 5]) # Show only first 5 records\n",
    "#print('x2 = ', x2[: 5])\n",
    "#print('x3 = ', x3[: 5])\n",
    "print('x1Norm = ',x1Norm[: 5])\n",
    "print('Y = ', Ynorm[: 5])\n",
    "print('m = ', m)\n",
    "print('Size of Newtrain: ', np.shape(df_Newtrain))\n",
    "size = np.shape(x1) # Check array dimensions\n",
    "print('Size of x1: ', size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc898a1c-ba0f-46f8-85cd-4c83521dd949",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_0 = np.ones((m, 1)) # Create a 99x1 array of ones\n",
    "\n",
    "X_1 = x1.reshape(m,1) # area, not scaled\n",
    "X_1Norm = x1Norm.reshape(m, 1) # area, scaled\n",
    "\n",
    "X_2 = x2.reshape(m, 1) # bedrooms\n",
    "X_3 = x3.reshape(m, 1) # bathrooms\n",
    "X_4 = x4.reshape(m, 1) # stories\n",
    "X_5 = x5.reshape(m, 1) # parking spots\n",
    "\n",
    "print('X_1 = ', X_1[:10])\n",
    "print('X_1Norm = ', X_1Norm[:10]) # first 10 rows of parking spots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb02b62b-daa4-4bb9-83fb-b4411b46f5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack column vectors X_0, X_1, X_2, X_3, X_4, X_5\n",
    "\n",
    "size = np.shape(df_Newtest)\n",
    "print(size)\n",
    "\n",
    "X_totNorm = np.hstack((X_0, X_1Norm, X_2, X_3, X_4, X_5))\n",
    "X_tot = np.hstack((X_0, X_1, X_2, X_3, X_4, X_5))\n",
    "\n",
    "print('X_tot = ')\n",
    "print(X_tot[:5])\n",
    "\n",
    "print('X_totNorm = ')\n",
    "print(X_totNorm[:5])\n",
    "\n",
    "size = np.shape(X_totNorm)\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83ad384-c260-4a3d-83c6-a903d1cc99c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = [0., 0., 0., 0., 0., 0.] # row vector for Theta initialized to 0\n",
    "iterations = 100;\n",
    "\n",
    "alpha1 = 0.01; # Comparing 3 different training rates\n",
    "alpha2 = 0.02;\n",
    "alpha3 = 0.042;\n",
    "\n",
    "size = np.shape(df_Newtest)\n",
    "print(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e80a83-ac05-4e5b-abab-afba261707de",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta1, cost_history1 = gradient_descent(X_tot, Y, theta, 0.00000001, iterations)\n",
    "thetaNorm, cost_historyNorm = gradient_descent(X_totNorm, Ynorm, theta, alpha1, iterations)\n",
    "\n",
    "#theta1, cost_history1 = gradient_descent(X_totNorm, Ynorm, theta, alpha1, iterations)\n",
    "#theta2, cost_history2 = gradient_descent(X_tot, Ynorm, theta, alpha2, iterations)\n",
    "#theta3, cost_history3 = gradient_descent(X_tot, Ynorm, theta, alpha3, iterations)\n",
    "size = np.shape(df_Newtest)\n",
    "print(size)\n",
    "\n",
    "print('Final value of theta1 =', theta1)\n",
    "print(cost_history1[: 10]) # Here the cost history just blows up without a small enough learning rate\n",
    "\n",
    "print('Final value of thetaNorm =', thetaNorm)\n",
    "print(cost_historyNorm[: 10])\n",
    "\n",
    "# print(cost_history2[: 10])\n",
    "# print(cost_history3[: 10])\n",
    "\n",
    "# print('Final value of theta2 =', theta2)\n",
    "# print('Final value of theta3 =', theta3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2697eae5-9134-4de1-a629-675ba547926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(range(1, iterations + 1),cost_history1, color='blue', label='a = 0.00000001')\n",
    "plt.plot(range(1, iterations + 1),cost_historyNorm, color='green', label='a = 0.01')\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,6)\n",
    "plt.grid()\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Cost (J)')\n",
    "plt.title('Convergence of gradient descent, normalized data')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bb34e9-4dc9-4a0f-84f3-d218f83b2375",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cost_historyNorm[99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a5e0d4-7566-439e-be18-ebe23430a744",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(df_Newtest)\n",
    "print(m)\n",
    "\n",
    "x1 = df_Newtest.values[:, 0] # get input values from first column\n",
    "x2 = df_Newtest.values[:, 1] \n",
    "x3 = df_Newtest.values[:, 2]\n",
    "x4 = df_Newtest.values[:, 3]\n",
    "x5 = df_Newtest.values[:, 4]\n",
    "Y = df_Newtest.values[:, 5]\n",
    "\n",
    "# Checking the size of df_Newtest\n",
    "size = np.shape(df_Newtest)\n",
    "print(size)\n",
    "\n",
    "# Used to scale the input data, the gradient descent converges faster this way\n",
    "Ynorm = np.multiply(Y,0.00001) # scales the price my Price*(10^-5)\n",
    "x1Norm = np.multiply(x1,0.001) # scales the area by Area*(10^-3)\n",
    "\n",
    "X_0 = np.ones((m, 1)) # Create a 99x1 array of ones\n",
    "\n",
    "X_1 = x1.reshape(m,1) # area, not scaled\n",
    "X_1Norm = x1Norm.reshape(m, 1) # area, scaled\n",
    "\n",
    "X_2 = x2.reshape(m, 1) # bedrooms\n",
    "X_3 = x3.reshape(m, 1) # bathrooms\n",
    "X_4 = x4.reshape(m, 1) # stories\n",
    "X_5 = x5.reshape(m, 1) # parking spots\n",
    "\n",
    "# Checking outputs\n",
    "print('Array X_1: ',X_1[: 5])\n",
    "print('Array X_5: ',X_5[: 5])\n",
    "print('Array Y: ',Y[: 5])\n",
    "print('Array X_1Norm size: ', np.shape(X_1Norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57c0b16-c6c6-4618-ba14-b1047743b6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#k = len(theta1)\n",
    "#print(k)\n",
    "\n",
    "#C = []\n",
    "#for i in k:\n",
    "#    C[i] = theta[i]*X_i\n",
    "#print(C[0])\n",
    "\n",
    "firstColumn = theta1[1]*X_1\n",
    "secondColumn = theta1[2]*X_2\n",
    "\n",
    "\n",
    "sum_list = []\n",
    "for (item1, item2) in zip(firstColumn, secondColumn):\n",
    "    sum_list.append(item1+item2)\n",
    "\n",
    "print(sum_list[: 5])\n",
    "\n",
    "print('Length of sum_list: ', len(sum_list),' and of size: ', np.shape(sum_list))\n",
    "print(firstColumn[: 5])\n",
    "print(secondColumn[: 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324c125b-244e-4075-9b9d-cbe36b1882c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is algorithm that evaluates the Test Data with the Training Model\n",
    "k = len(theta1) # theta1 and thetaNorm are the same length\n",
    "print('Length of theta1: ',k)\n",
    "print(df_Newtest.head())\n",
    "print('Ynorm: ',Ynorm[: 5])\n",
    "\n",
    "C = [0,0,0,0,0,0]\n",
    "CNorm = [0,0,0,0,0,0]\n",
    "\n",
    "XNorm = [X_0, X_1Norm, X_2, X_3, X_4, X_5]\n",
    "X = [X_0, X_1, X_2, X_3, X_4, X_5]\n",
    "\n",
    "# Inserting test variable values into Training Model. theta1 and thetaNorm are calculated from the training iterations\n",
    "for i in range(len(theta1)):\n",
    "    #print('i = ',i)\n",
    "    B = theta1[i]*X[i]\n",
    "    BNorm = thetaNorm[i]*XNorm[i]\n",
    "    C[i] = B\n",
    "    CNorm[i] = BNorm\n",
    "\n",
    "# Total value for normalized data\n",
    "# print(CNorm[0])\n",
    "CNorm_tot = CNorm[0]+CNorm[1]+CNorm[2]+CNorm[3]+CNorm[4]+CNorm[5]\n",
    "print('CNorm_tot = ', np.shape(CNorm_tot), CNorm_tot[: 5])\n",
    "\n",
    "# Total value for regular data\n",
    "C_tot = C[0]+C[1]+C[2]+C[3]+C[4]+C[5]\n",
    "print('C_tot = ', np.shape(C_tot), C_tot[: 5])\n",
    "\n",
    "# Arrays to store % difference between test Prices and actual Prices\n",
    "diff = np.zeros(len(C_tot))\n",
    "diffNorm = diff \n",
    "\n",
    "# Calculating % difference\n",
    "for i in range(len(C_tot)-1):\n",
    "    B = (C_tot[i]-Y[i])/Y[i]\n",
    "    BNorm = (CNorm_tot[i]-Ynorm[i])/Ynorm[i]\n",
    "    diff[i] = B*100\n",
    "    diffNorm[i] = BNorm*100\n",
    "\n",
    "print('Differences: ', np.shape(diff), diff[: 5])\n",
    "print('Differences (normalized): ', np.shape(diffNorm), diffNorm[: 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacfc72e-3984-40d5-8cc8-6ff934222130",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(1,len(diff)+1),diff)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,6)\n",
    "plt.grid()\n",
    "plt.xlabel('Sample #')\n",
    "plt.ylabel('% difference')\n",
    "plt.title('Training model differences From actual values')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3913d213-0742-43b7-89e5-973acab59e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(1,len(diffNorm)+1),diffNorm)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,6)\n",
    "plt.grid()\n",
    "plt.xlabel('Sample #')\n",
    "plt.ylabel('% difference')\n",
    "plt.title('Training model differences From actual values (Area and Prices Normalized)')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518fb378-c665-41f3-ade1-fc1f4723397b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make this a for loop that runs through the validation data set. this will also tell the loss from the predicted value to the actual value?\n",
    "#price = df_Newtest.pop('price')\n",
    "#print(price[:5])\n",
    "#row1 = df_Newtest[: 2]\n",
    "#print(row1)\n",
    "#validation1 = thetaNorm*df_Newtest\n",
    "# print(validation[: 2])\n",
    "#k = 5;\n",
    "#for i in m:\n",
    "#  for j in k:\n",
    "#    print(df_Newtest[i k]) ??\n",
    "\n",
    "#print(np.shape(df_Newtest))\n",
    "#print(df_Newtest[2:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fddb1e9-8510-468f-b420-066d60931191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section introduces the normalization and standardization section of the HW\n",
    "# Here we can see that except for area, all the columns have small integer values.\n",
    "#So it is extremely important to rescale the variables so that they have a comparable s\n",
    "#If we don't have comparable scales, then some of the coefficients as obtained by fitti\n",
    "#This might become very annoying at the time of model evaluation.\n",
    "##So it is advised to use standardization or normalization so that the units of the coef\n",
    "\n",
    "#As you know, there are two common ways of rescaling:\n",
    "#1. Min-Max scaling\n",
    "#2. Standardisation (mean-0, sigma-1)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # Not sure what this means\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# define standard scaler\n",
    "#scaler = StandardScaler()\n",
    "\n",
    "# define MinMax scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "df_Newtrain[num_vars] = scaler.fit_transform(df_Newtrain[num_vars])\n",
    "df_Newtrain.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88bb080-edae-49a0-91e8-e13d17dac3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_Newtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24fbc19-e44b-4ca0-b183-945bcc2b98e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure what these do but it seems that the dataset is just split up into two different submatrices? \n",
    "\n",
    "y_Normtrain = df_Newtrain.pop('price') # takes out the price column of the matrix df_Newtrain and assigns it to y_Newtrain\n",
    "X_Normtrain = df_Newtrain # A matrix containing the remaining matrix df_Newtrain after removal with .pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2465f42-d974-41d7-926d-57614601ffd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Normtrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7bf339-a1d9-46f9-b345-65ef882fa386",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_Normtrain.head() # Price column matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02859d0-7010-429a-b4e0-d9118738af4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = y_Normtrain.values # get input values from price column, row vector containing price values\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b08e619-e2cf-40e7-951e-47639478f40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X0 = df_Newtrain.values[:, 0] # get input values from first column, this does not make sense because df_Normtrain is not defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0483f3-b4e7-41af-88be-b593e0dcd1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aa80ad-25a8-4c20-b96f-17500632ecdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
